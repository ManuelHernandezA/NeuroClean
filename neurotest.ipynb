{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3b1cb0e",
   "metadata": {},
   "source": [
    "## ML Papers with databases\n",
    "\n",
    "CBH-MIT (https://physionet.org/content/chbmit/1.0.0):\n",
    "- Ben Messaoud & Chavéz, \"Random Forest classifier for EEG-based seizure prediction\" (arXiv 2021). Need to apply random forest, no code available. Easy feasibility\n",
    "- Usman et al., \"Epileptic seizure prediction using scalp EEG\" (2021). No code available, but libraries and implementation specified in the paper, uses SVM and Random Forest, classical feature extraction. Easy to medium feasibility.\n",
    "\n",
    "CRCNS (https://crcns.org/data-sets/hc/hc-27 ?):\n",
    "- Agarwal et al., \"Spatially Distributed Local Fields in the Hippocampus\" (2014, PLoS Biology). Not completely sure about which dataset from CRCNS its using. Results are rather too theoretical to be precisely assessed. Implementation not available, code not available. Very hard feasibility.\n",
    "\n",
    "\n",
    "Kaggle dataset from the American Society for seizure prediction (https://www.kaggle.com/competitions/seizure-prediction/data?select=Patient_2):\n",
    "- Ben Messaoud & Chavéz, \"Random Forest classifier for EEG-based seizure prediction\" (arXiv 2021). Need to apply random forest, no code available. Easy feasibility\n",
    "- Also code available specifically for this challenge in the Kaggle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67b6d2d",
   "metadata": {},
   "source": [
    "## Dataset 1: Numa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a1370c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mat73\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# Load LFP data from DanCause Laboratory\n",
    "data = mat73.loadmat('E:\\\\Dancause_yellow_hd\\\\export\\\\lfp_data_20180612Y.mat')\n",
    "\n",
    "streams = data[\"lfp_data2\"][\"streams\"]\n",
    "# This is the clock frequency of the data from the Laboratory, you may change it to the sampling frequency of the LFP\n",
    "basefs = 4.8828125*10**3\n",
    "\n",
    "# This was used to calculate the ratio of clock and sampling frequency, it is 1 if the clock is the same as the sampling rate \n",
    "for y in streams:\n",
    "    if float(streams[y]['fs']) != basefs:\n",
    "        streams[y]['ratio'] = float(basefs/streams[y]['fs'])\n",
    "    else:\n",
    "        streams[y]['ratio'] = 1\n",
    "\n",
    "meta = dict()\n",
    "meta['ratio'] = streams['LFP1']['ratio']\n",
    "meta['fs'] = streams['LFP1']['fs']\n",
    "\n",
    "# Save both LFP as two separate files\n",
    "lfp1_data = streams['LFP1']['data']\n",
    "lfp2_data  = streams['LFP2']['data']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4eafb012",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Bandpassing: 128it [00:19,  6.61it/s] | 0/4 [00:00<?, ?it/s]\n",
      "NeuroClean processing:  25%|██▌       | 1/4 [00:19<00:58, 19.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Power of components removed by DSS: 0.04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NeuroClean processing:  75%|███████▌  | 3/4 [08:00<02:32, 152.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..............................................................................................................................."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NeuroClean processing:  75%|███████▌  | 3/4 [1:42:17<34:05, 2045.93s/it]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "DBSCAN.fit() missing 1 required positional argument: 'X'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mneuroclean\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnc\u001b[39;00m\n\u001b[32m      3\u001b[39m ncp = nc.NeuroClean(random_state=\u001b[32m42\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[43mncp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlfp1_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbasefs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m lfp1_data = ncp.data\n\u001b[32m      7\u001b[39m ncp.preprocess(lfp2_data, basefs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\important_documents\\TFG\\NeuroClean\\neuroclean.py:65\u001b[39m, in \u001b[36mNeuroClean.preprocess\u001b[39m\u001b[34m(self, data, frequency, class_info, substeps)\u001b[39m\n\u001b[32m     63\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mClusterMARA\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m substeps:\n\u001b[32m     64\u001b[39m     \u001b[38;5;28mself\u001b[39m.__ica()\n\u001b[32m---> \u001b[39m\u001b[32m65\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__clustermara\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     66\u001b[39m     pbar.update(\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\important_documents\\TFG\\NeuroClean\\neuroclean.py:242\u001b[39m, in \u001b[36mNeuroClean.__clustermara\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    240\u001b[39m clustering = DBSCAN(eps=\u001b[32m2\u001b[39m, min_samples=\u001b[32m2\u001b[39m)\n\u001b[32m    241\u001b[39m mara_features = \u001b[38;5;28mself\u001b[39m.__mara(\u001b[38;5;28mself\u001b[39m.ica, \u001b[38;5;28mself\u001b[39m.components, \u001b[38;5;28mself\u001b[39m.fs)\n\u001b[32m--> \u001b[39m\u001b[32m242\u001b[39m fitted = \u001b[43mclustering\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    243\u001b[39m rejected = np.where(fitted.labels_ == -\u001b[32m1\u001b[39m)[\u001b[32m0\u001b[39m]\n\u001b[32m    244\u001b[39m \u001b[38;5;28mself\u001b[39m.components[:, rejected] = \u001b[32m0\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\sklearn\\base.py:1365\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1358\u001b[39m     estimator._validate_params()\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1361\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1362\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1363\u001b[39m     )\n\u001b[32m   1364\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: DBSCAN.fit() missing 1 required positional argument: 'X'"
     ]
    }
   ],
   "source": [
    "import neuroclean as nc\n",
    "\n",
    "ncp = nc.NeuroClean(random_state=42)\n",
    "\n",
    "ncp.preprocess(lfp1_data, basefs)\n",
    "lfp1_data = ncp.data\n",
    "ncp.preprocess(lfp2_data, basefs)\n",
    "lfp2_data = ncp.data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180ce77d",
   "metadata": {},
   "source": [
    "## Dataset 2: Kaggle Competition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fcd275",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('D:\\\\important_documents\\\\KaggleSeizure'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b100f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "#from keras import backend as K\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import scipy.io\n",
    "from scipy.signal import spectrogram\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4065477",
   "metadata": {},
   "outputs": [],
   "source": [
    "interictal_tst = 'D:/important_documents/KaggleSeizure/Patient_1/Patient_1/Patient_1_interictal_segment_0001.mat'\n",
    "preictal_tst = 'D:/important_documents/KaggleSeizure/Patient_1/Patient_1/Patient_1_preictal_segment_0001.mat'\n",
    "interictal_data = scipy.io.loadmat(interictal_tst)\n",
    "preictal_data = scipy.io.loadmat(preictal_tst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079dbe74",
   "metadata": {},
   "outputs": [],
   "source": [
    "interictal_array = interictal_data['interictal_segment_1'][0][0][0]\n",
    "preictal_array = preictal_data['preictal_segment_1'][0][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ca70c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ncp = nc.NeuroClean(random_state=42)\n",
    "\n",
    "ncp.preprocess(interictal_array)\n",
    "interictal_array = ncp.data\n",
    "ncp.preprocess(preictal_array)\n",
    "preictal_array = ncp.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695df334",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = list(range(10000))\n",
    "for i in l[::5000]:\n",
    "    print('Interictal')\n",
    "    i_secs = interictal_array[0][i:i+5000]\n",
    "    i_f, i_t, i_Sxx = spectrogram(i_secs, fs=5000, return_onesided=False)\n",
    "    i_SS = np.log1p(i_Sxx)\n",
    "    plt.imshow(i_SS[:] / np.max(i_SS), cmap='gray')\n",
    "    plt.show()\n",
    "    print('Preictal')\n",
    "    p_secs = preictal_array[0][i:i+5000]\n",
    "    p_f, p_t, p_Sxx = spectrogram(p_secs, fs=5000, return_onesided=False)\n",
    "    p_SS = np.log1p(p_Sxx)\n",
    "    plt.imshow(p_SS[:] / np.max(p_SS), cmap='gray')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d46893",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_X = []\n",
    "all_Y = []\n",
    "\n",
    "types = ['Patient_1_interictal_segment', 'Patient_1_preictal_segment']\n",
    "\n",
    "for i,typ in enumerate(types):\n",
    "    # Looking at 18 files for each event for a balanced dataset\n",
    "    for j in range(18):\n",
    "        fl = '/kaggle/input/seizure-prediction/Patient_1/Patient_1/{}_{}.mat'.format(typ, str(j + 1).zfill(4))\n",
    "        data = scipy.io.loadmat(fl)\n",
    "        k = typ.replace('Patient_1_', '') + '_'\n",
    "        d_array = data[k + str(j + 1)][0][0][0]\n",
    "        lst = list(range(3000000))  # 10 minutes\n",
    "        for m in lst[::5000]:\n",
    "            # Create a spectrogram every 1 second\n",
    "            p_secs = d_array[0][m:m+5000]\n",
    "            p_f, p_t, p_Sxx = spectrogram(p_secs, fs=5000, return_onesided=False)\n",
    "            p_SS = np.log1p(p_Sxx)\n",
    "            arr = p_SS[:] / np.max(p_SS)\n",
    "            all_X.append(arr)\n",
    "            all_Y.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d933e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffling the data\n",
    "dataset = list(zip(all_X, all_Y))\n",
    "random.shuffle(dataset)\n",
    "all_X,all_Y = zip(*dataset)\n",
    "print(len(all_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28ac062",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting data into train/test, leaving only 600 samples for testing\n",
    "x_train = np.array(all_X[:21000])\n",
    "y_train = np.array(all_Y[:21000])\n",
    "x_test = np.array(all_X[21000:])\n",
    "y_test = np.array(all_Y[21000:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e331dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_classes = 2\n",
    "epochs = 30\n",
    "img_rows, img_cols = 256, 22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5035f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "input_shape = (img_rows, img_cols, 1)\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af3baf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes) \n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92bff08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense, Input, GlobalAveragePooling2D, Multiply, Reshape, Permute\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "# Attention mechanism block (Self-attention)\n",
    "def attention_block(inputs):\n",
    "    channels = inputs.shape[-1]\n",
    "    attention = GlobalAveragePooling2D()(inputs)\n",
    "    attention = Dense(channels // 8, activation='relu')(attention)\n",
    "    attention = Dense(channels, activation='sigmoid')(attention)\n",
    "    attention = Reshape((1, 1, channels))(attention)\n",
    "    attention = Multiply()([inputs, attention])\n",
    "    return attention\n",
    "\n",
    "# Modify input_shape to match your data shape (with 1 channel)\n",
    "input_shape = (256, 22, 1)\n",
    "\n",
    "inputs = Input(shape=input_shape)\n",
    "x = Conv2D(32, kernel_size=(5, 5), activation='relu')(inputs)\n",
    "x = Conv2D(32, kernel_size=(3, 3), activation='relu')(x)\n",
    "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "x = Dropout(0.25)(x)\n",
    "\n",
    "# Apply attention mechanism after convolutional layers\n",
    "x = attention_block(x)\n",
    "\n",
    "# Flatten, Dense, Dropout, and Output layers (same as before)\n",
    "x = Flatten()(x)\n",
    "x = Dense(32, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "outputs = Dense(2, activation='sigmoid')(x)\n",
    "\n",
    "model = Model(inputs, outputs)\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d2d934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad39a933",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
